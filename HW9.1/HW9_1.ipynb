{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW9.1 Sentiment classification with LSTMs and MLPs"
      ],
      "metadata": {
        "id": "fW7ytRZbFJQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework we will work with the tf.keras framework. To simplify things, we will use the keras built-in dataset of IMDB movie reviews. To use the Colab GPUs, let's run this whole exercise in Colab, and then download it to your laptop and push it to the github repo under your own branch, within a folder named HW9.1."
      ],
      "metadata": {
        "id": "HY8LQwaCxPDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Notebook on bi-LSTM movie review classification in Colab"
      ],
      "metadata": {
        "id": "N7M4xG8UE424"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will get started with the code example found in the Keras documentation. Head to this page, and then open the notebook in Colab.\n",
        "\n",
        "https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
      ],
      "metadata": {
        "id": "VnnWERLADof3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WINHWG6v9dBn"
      },
      "source": [
        "### Bidirectional LSTM on IMDB\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2020/05/03<br>\n",
        "**Last modified:** 2020/05/03<br>\n",
        "**Description:** Train a 2-layer bidirectional LSTM on the IMDB movie review sentiment classification dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-2qz0xi9dBr"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HApTSoC9dBr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "max_features = 20000  # Only consider the top 20k words\n",
        "# vocabulary size = |V| = 20000\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "# shorter than 200: add 0s\n",
        "# longer than 200: truncate\n",
        "\n",
        "# Set random seed to make the results replicable\n",
        "# For Python, NumPy, and TensorFlow\n",
        "keras.utils.set_random_seed(297)\n",
        "# https://keras.io/examples/keras_recipes/reproducibility_recipes/\n",
        "# tensorflow.config.experimental.enable_op_determinism()\n",
        "# some models can not reproduce the same output\n",
        "# takes too much time to run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYI-z2Vb9dBs"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXao9Ohq9dBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1028e3a-fbb1-4e42-e940-432ce9e91a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, None, 128)         98816     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 128)               98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2757761 (10.52 MB)\n",
            "Trainable params: 2757761 (10.52 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # input layer, input sequences can have variable lengths\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs) # embedding layer e.g., (25000, 200, 128), flexible batch size and sequence length\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # each forward and backward pass consists of an LSTM layer with 64 units\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x) # without \"return_sequences=True\": produce a single output for the entire sequence\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # a dense layer with one unit and a sigmoid activation function: binary classifier\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "# https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_WDJVfj9dBt"
      },
      "source": [
        "#### Load the IMDB movie review sentiment data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5iwh_1h9dBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08c97ff-1449-4591-d078-2cb078bc6c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
        "    num_words=max_features\n",
        ")\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_tv-Jid9dBu"
      },
      "source": [
        "#### Train and evaluate the model\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/bidirectional-lstm-imdb) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/bidirectional_lstm_imdb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtEJgH489dBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8106443-2fd0-400b-ea33-af19fae72f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 73s 81ms/step - loss: 0.3979 - accuracy: 0.8163 - val_loss: 0.3637 - val_accuracy: 0.8445\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 56s 72ms/step - loss: 0.2186 - accuracy: 0.9166 - val_loss: 0.3402 - val_accuracy: 0.8648\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea468668160>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\n",
        "# each epoch starts with the model's parameters as they were at the end of the previous epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answers"
      ],
      "metadata": {
        "id": "oK-RkopFw8Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run through all the cells and try to understand what each cell is doing. Especially pay attention to the output of `model.summary()`. Look at the training data. How are they represented?\n",
        "\n",
        "Record the parameter count of this model. Also record the final accuracy of the model."
      ],
      "metadata": {
        "id": "d0PE8IK9wp2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of `model.summary()` shows the type of layers as well as the corresponding output shape and the number of parameters.          \n",
        "Param # for Embedding = `vocab_size x embedding_dim` = `max_features × 128` = `20000 × 128` = `2560000`         \n",
        "Param # for the 1st Bidirectional = `num_direction × num_FFNNs_per_unit × [(num_units + input_size) × num_units + num_units]` = `2 × 4 x [(64 + 128) × 64 + 64]` = `98816`      \n",
        "Param # for the 2nd Bidirectional = `num_direction × num_FFNNs_per_unit × [(num_units + input_size) × num_units + num_units]` = `2 × 4 x [(64 + 128) × 64 + 64]` = `98816`         \n",
        "Param # for Dense = `(prev_layer_output_size + 1) × units` = `(128 + 1) × 1` = `129`"
      ],
      "metadata": {
        "id": "XYeywRea2z8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYwdcrbH4MW7",
        "outputId": "da951b67-9f6a-4a32-df63-216f3485fc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training data are represented in the form of a 25000 by 200 matrix, 25000 is the number of sequences in the training set and 200 is the number of words to consider for each movie review. Given a sequence of words, an LSTM processes one word at a time. The embedding layer converts each word in the vocabulary (size = 20000) to a vector of size 128, and these word vectors are sequentially fed into the LSTM for sequence processing."
      ],
      "metadata": {
        "id": "C5xIR5zoFQO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameter count of this model is 2757761.             \n",
        "The final accuracy (val_accuracy of the last epoch) of the model is 0.8648."
      ],
      "metadata": {
        "id": "SIxxL_z5FBca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: hyperparameters"
      ],
      "metadata": {
        "id": "Dj7AyK75FiQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try change the batch size to 16 and 64. How does the accuracy change?"
      ],
      "metadata": {
        "id": "C3ig6wl4FlyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinitialize the model\n",
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # input layer\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs) # embedding layer\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # each forward and backward pass consists of an LSTM layer with 64 units\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x) # without \"return_sequences=True\": produce a single output for the entire sequence\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # a dense layer with one unit and a sigmoid activation function: binary classifier\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=16, epochs=2, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM8LTJ2AF2Ef",
        "outputId": "1c81b1d3-da77-46b2-d6dd-ef5b3a68ebbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1563/1563 [==============================] - 105s 63ms/step - loss: 0.4461 - accuracy: 0.7930 - val_loss: 0.3579 - val_accuracy: 0.8543\n",
            "Epoch 2/2\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.2591 - accuracy: 0.9000 - val_loss: 0.3885 - val_accuracy: 0.8479\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3d65842b0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When batch_size=16, the final accuracy is 0.8479, which is lower than that of the model with a batch size of 32. This value is also lower than the validation accuracy of the first epoch, suggesting this model is likely to be overfitting."
      ],
      "metadata": {
        "id": "5CQhuClgIQ2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # input layer\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs) # embedding layer\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # each forward and backward pass consists of an LSTM layer with 64 units\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x) # without \"return_sequences=True\": produce a single output for the entire sequence\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # a dense layer with one unit and a sigmoid activation function: binary classifier\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=2, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "AV99heJEF3L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac61aa21-662d-4660-882b-2e8508a13e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "391/391 [==============================] - 51s 111ms/step - loss: 0.3866 - accuracy: 0.8223 - val_loss: 0.3083 - val_accuracy: 0.8710\n",
            "Epoch 2/2\n",
            "391/391 [==============================] - 25s 64ms/step - loss: 0.1958 - accuracy: 0.9285 - val_loss: 0.3590 - val_accuracy: 0.8669\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3f051e8f0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When batch_size=64, the final accuracy is 0.8669, which is slightly higher than that of the model with a batch size of 32. However, the validation accuracy of the 2nd epoch is still lower than the previous result, showing signs of overfitting."
      ],
      "metadata": {
        "id": "z4I2G1FdIlIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While holding the batch size 32, experiment with training the model longer with a few more epochs. How does the final accuracy change?"
      ],
      "metadata": {
        "id": "e5ijPpiIF7t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # input layer\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs) # embedding layer\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # each forward and backward pass consists of an LSTM layer with 64 units\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x) # without \"return_sequences=True\": produce a single output for the entire sequence\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # a dense layer with one unit and a sigmoid activation function: binary classifier\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "VBvi-RVEGJbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4144b6-6d41-4e36-d566-e34ee40a2693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 74s 87ms/step - loss: 0.4038 - accuracy: 0.8160 - val_loss: 0.3115 - val_accuracy: 0.8711\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 39s 49ms/step - loss: 0.2087 - accuracy: 0.9227 - val_loss: 0.3749 - val_accuracy: 0.8476\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 36s 47ms/step - loss: 0.1229 - accuracy: 0.9579 - val_loss: 0.3860 - val_accuracy: 0.8606\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.0997 - accuracy: 0.9655 - val_loss: 0.5105 - val_accuracy: 0.8546\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 33s 43ms/step - loss: 0.0859 - accuracy: 0.9694 - val_loss: 0.4800 - val_accuracy: 0.8517\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3ded53e20>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the number of epochs increases, there appears to be a monotonic increase in the training accuracy as the model is given more opportunities to learn the data, while the validation accuracy shows a fluctuating decreasing trend. In the last epoch, the final training accuracy is almost 0.97 and the final validation accuracy is 0.8517. The values indicate that the model is likely to be overfitting, and early stopping should be adopted, which means training can stop after the first epoch, as it generates the highest validation accuracy (0.8711)."
      ],
      "metadata": {
        "id": "U8QbHBRWojtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record the best accuracy and the settings you obtained them with. Use these settings going forward."
      ],
      "metadata": {
        "id": "QyworTFDGOPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the validation accuracy of the last epoch:    \n",
        "\n",
        "*   batch_size=32, epochs=2: 0.8648\n",
        "*   batch_size=16, epochs=2: 0.8479\n",
        "*   batch_size=64, epochs=2: 0.8669\n",
        "*   batch_size=32, epochs=5: 0.8517    \n",
        "\n",
        "So, for the following tasks, the settings should be batch_size=64 and epochs=2.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sYASYXiEvBOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: modify LSTM architecture"
      ],
      "metadata": {
        "id": "E-iaP43aGRW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original model has two layers of bi-directional LSTMs. Change it to only one and train again. How does the accuracy change? Write down any observations."
      ],
      "metadata": {
        "id": "s2xJRd5CGZVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Add 1 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x) # binary classification - produce output only at the final time step, and not the full sequence\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qJWkrmjRI6mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f72a489-1e8c-4a8a-f3b6-f963f1af721c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirecti  (None, 128)               98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2658945 (10.14 MB)\n",
            "Trainable params: 2658945 (10.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=2, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "7B9m953u8YmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19de354-a4e9-4778-f693-7576f1a06e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "391/391 [==============================] - 44s 102ms/step - loss: 0.4107 - accuracy: 0.8087 - val_loss: 0.3136 - val_accuracy: 0.8711\n",
            "Epoch 2/2\n",
            "391/391 [==============================] - 18s 47ms/step - loss: 0.2076 - accuracy: 0.9237 - val_loss: 0.3244 - val_accuracy: 0.8678\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3dae874f0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to the LSTM model with two bi-directional layers, since this model has fewer layers, the training time is slightly shorter. The training accuracies are also slightly lower than those of the original model, as this model is less complicated, making it harder to capture the information in the dataset. However, the validation accuracies are higher, even though the differences are not significant, which means the model with one bi-directional layer may be more generalized, thus performing better."
      ],
      "metadata": {
        "id": "Xt-us_Vpt-fO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: use MLP (dense layers)"
      ],
      "metadata": {
        "id": "M71oz5bqGakP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's swap out the bi-LSTM layers with two dense layers with 64 hidden units. (refer to keras Functional API documentation for how to add a dense layer). Try to compile the model and look at the summary. How is it different from the LSTM model? How does the parameter count differ?\n",
        "\n",
        "Now try to train the MLP model. Does it run?\n",
        "\n",
        "If it doesn't run, can you explain why by looking at the comparison between the two model summary outputs?\n",
        "\n",
        "Hint: in addition to adding two dense layers, you might need to also change the input dimension specification to maxlen, and do a Flatten operation between the embedding layer and the dense layer.\n",
        "\n",
        "Try making changes to the MLP network so that you can train a model with it too."
      ],
      "metadata": {
        "id": "ycMdVkSXGdfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Add 2 dense layers with 64 hidden units\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "844Yl8DU9mGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9fa3323-a487-4c70-a51c-7e5cef254df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, None, 64)          8256      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, None, 64)          4160      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, None, 1)           65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2572481 (9.81 MB)\n",
            "Trainable params: 2572481 (9.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MLP model is different from the LSTM model as two Dense layers replace the two Bidirectional layers, and some layers have different output shapes (the last two layers of this model are 3D while the last two layers of LSTM are 2D), and different numbers of parameters compared to the original model.     \n",
        "Param # for the 1st Dense = `(num_input_units + 1) × num_output_units` = `(128 + 1) × 64` = `8256`    \n",
        "Param # for the 2nd Dense = `(num_input_units + 1) × num_output_units` = `(64 + 1) × 64` = `4160`    \n"
      ],
      "metadata": {
        "id": "Rvv-ab8v0NDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "# model.fit(x_train, y_train, batch_size=64, epochs=2, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "QOJFvcuh_NCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does not run.          \n",
        "ValueError: `logits` and `labels` must have the same shape, received ((None, 200, 1) vs (None,))."
      ],
      "metadata": {
        "id": "QT1yefhV_dYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does not run because the layers are not in the correct shape. MLP does not process each word at a time, so the input layer needs to have a specified dimension of `200`. MLP processes each word in parallel after embedding, and the embedded vectors need to be flattened into a one-dimensional vector, resulting in a flattened vector size of `128` × `200` = `25600`, which is further processed through dense layers."
      ],
      "metadata": {
        "id": "_ejNuCspHKk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(maxlen,), dtype=\"int32\") # change the input dimension specification to \"maxlen\"\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Flatten operation between the embedding layer and the dense layer\n",
        "x = layers.Flatten()(x) # reshapes the multidimensional input data into a one-dimensional array (vector) without modifying the actual data\n",
        "# Add 2 dense layers with 64 hidden units\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "WDJViUBC_mSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0f5fa4-0334-425e-96c6-9734c94448a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, 200, 128)          2560000   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25600)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                1638464   \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4202689 (16.03 MB)\n",
            "Trainable params: 4202689 (16.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Param # for the 1st Dense = `(num_input_units + 1) × num_output_units` = `(25600 + 1) × 64` = `1638464`    "
      ],
      "metadata": {
        "id": "OBKXaO_uUO4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=2, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "52sc1TCsCFkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c73333-7234-4154-f59c-9e82159e310b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "391/391 [==============================] - 30s 72ms/step - loss: 0.3979 - accuracy: 0.8057 - val_loss: 0.2987 - val_accuracy: 0.8731\n",
            "Epoch 2/2\n",
            "391/391 [==============================] - 12s 30ms/step - loss: 0.0608 - accuracy: 0.9790 - val_loss: 0.4397 - val_accuracy: 0.8417\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3d635c280>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrap up"
      ],
      "metadata": {
        "id": "MKiVSQEoGhsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you got the MLP training running, play around with it to get the best accuracy. Report the final accuracy and compare it with the previous models."
      ],
      "metadata": {
        "id": "u8gOKDTWGkKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(maxlen,), dtype=\"int32\") # change the input dimension specification to \"maxlen\"\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Flatten operation between the embedding layer and the dense layer\n",
        "x = layers.Flatten()(x) # reshapes the multidimensional input data into a one-dimensional array (vector) without modifying the actual data\n",
        "# Add 2 dense layers with 64 hidden units\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3) # stop the training when there is no improvement in the val_accuracy for three consecutive epochs\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=16, epochs=10, validation_data=(x_val, y_val), callbacks=[callback])"
      ],
      "metadata": {
        "id": "qNBO8IioChl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd254af-a81b-41e0-8a81-20ad95a967a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 46s 28ms/step - loss: 0.3732 - accuracy: 0.8247 - val_loss: 0.3063 - val_accuracy: 0.8679\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0508 - accuracy: 0.9836 - val_loss: 0.5468 - val_accuracy: 0.8291\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0154 - accuracy: 0.9947 - val_loss: 0.7277 - val_accuracy: 0.8422\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0144 - accuracy: 0.9950 - val_loss: 0.7326 - val_accuracy: 0.8433\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3dffff9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(maxlen,), dtype=\"int32\") # change the input dimension specification to \"maxlen\"\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Flatten operation between the embedding layer and the dense layer\n",
        "x = layers.Flatten()(x) # reshapes the multidimensional input data into a one-dimensional array (vector) without modifying the actual data\n",
        "# Add 2 dense layers with 64 hidden units\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3) # stop the training when there is no improvement in the val_accuracy for three consecutive epochs\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val), callbacks=[callback])"
      ],
      "metadata": {
        "id": "uPKtbaopuYvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332ad7dd-2d90-4e73-b438-90488e5ed406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 36s 45ms/step - loss: 0.3775 - accuracy: 0.8221 - val_loss: 0.2991 - val_accuracy: 0.8742\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.0504 - accuracy: 0.9836 - val_loss: 0.4345 - val_accuracy: 0.8482\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.7239 - val_accuracy: 0.8495\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.0055 - accuracy: 0.9979 - val_loss: 0.8953 - val_accuracy: 0.8323\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3df8cdd20>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(maxlen,), dtype=\"int32\") # change the input dimension specification to \"maxlen\"\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Flatten operation between the embedding layer and the dense layer\n",
        "x = layers.Flatten()(x) # reshapes the multidimensional input data into a one-dimensional array (vector) without modifying the actual data\n",
        "# Add 2 dense layers with 64 hidden units\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3) # stop the training when there is no improvement in the val_accuracy for three consecutive epochs\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val), callbacks=[callback])"
      ],
      "metadata": {
        "id": "vjyTLtJwoGco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8f0f6d-54c8-4ec9-9deb-68ee1ddf0087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 28s 70ms/step - loss: 0.3935 - accuracy: 0.8084 - val_loss: 0.3019 - val_accuracy: 0.8710\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.0578 - accuracy: 0.9805 - val_loss: 0.4353 - val_accuracy: 0.8462\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 11s 27ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.6699 - val_accuracy: 0.8547\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 1.6316e-04 - accuracy: 1.0000 - val_loss: 0.7123 - val_accuracy: 0.8578\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3dffc1e10>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(maxlen,), dtype=\"int32\") # change the input dimension specification to \"maxlen\"\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Flatten operation between the embedding layer and the dense layer\n",
        "x = layers.Flatten()(x) # reshapes the multidimensional input data into a one-dimensional array (vector) without modifying the actual data\n",
        "# Add 2 dense layers with 64 hidden units\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=16, epochs=1, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVRKkwsi5ec8",
        "outputId": "a872c2bd-8025-4e12-8556-d12de9cab2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3691 - accuracy: 0.8262 - val_loss: 0.3065 - val_accuracy: 0.8688\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3d6e85930>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(maxlen,), dtype=\"int32\") # change the input dimension specification to \"maxlen\"\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Flatten operation between the embedding layer and the dense layer\n",
        "x = layers.Flatten()(x) # reshapes the multidimensional input data into a one-dimensional array (vector) without modifying the actual data\n",
        "# Add 2 dense layers with 64 hidden units\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=1, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhQU6J8y443u",
        "outputId": "8b557296-b36c-44b6-f262-bb65305c7b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 39s 47ms/step - loss: 0.3864 - accuracy: 0.8127 - val_loss: 0.3040 - val_accuracy: 0.8726\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3bd55add0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(maxlen,), dtype=\"int32\") # change the input dimension specification to \"maxlen\"\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Flatten operation between the embedding layer and the dense layer\n",
        "x = layers.Flatten()(x) # reshapes the multidimensional input data into a one-dimensional array (vector) without modifying the actual data\n",
        "# Add 2 dense layers with 64 hidden units\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=1, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhiYwLHs5o8C",
        "outputId": "74540481-7ad8-49f6-a446-49ec9c1bdd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 29s 71ms/step - loss: 0.4120 - accuracy: 0.7912 - val_loss: 0.3046 - val_accuracy: 0.8700\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ea3dd1dcfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the best accuracy, the first three models have a batch size of 16, 32, and 64 respectively, with 10 epochs. An early stopping criterion is added to stop the training when there is no improvement in the `val_accuracy` for three consecutive epochs to avoid overfitting. All three models stop training after four epochs, and all achieve the highest validation accuracy after the first epoch. So, the following three models have a batch size of 16, 32, and 64 respectively, with only 1 epoch. The model with a batch size of 32 achieves the highest final validation accuracy of 0.8726."
      ],
      "metadata": {
        "id": "tUgJ4jrrH8He"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This best MLP model achieves higher final validation accuracy than the best LSTM, which is the one with one bi-directional layer (batch_size=64, epoch=2). However, it is possible that with further hyperparameter tuning, LSTM can achieve better results, since MLP has 4202689 parameters, which is much higher than that of the LSTM model, so it is even more likely to overfit. It is also important to note that, the accuracies are not significantly different despite the changes in batch size and epoch, so maybe more data are required or less complicated models should be considered in order to better address the overfitting issue."
      ],
      "metadata": {
        "id": "tOIbT-rFU68A"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}