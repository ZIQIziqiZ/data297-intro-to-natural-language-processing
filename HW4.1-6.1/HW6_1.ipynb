{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuEHwhaEeX_C"
      },
      "source": [
        "# HW6.1 Text classification with w2v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4qG2I_0eX_G"
      },
      "source": [
        "We will use the same data set Triage as last two weeks. Here is what's in this assignment.\n",
        "\n",
        "1. we will explore text classification with pre-trained w2v embeddings with logistic regression.\n",
        "\n",
        "2. we will explore text classification with w2v embeddings trained on the Triage training dataset and then test it on the dev dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpfaVDCxeX_H"
      },
      "source": [
        "## PART I: Using pre-trained w2v embeddings for text classification\n",
        "\n",
        "For data loading, you should use the same code as last time so that you can obtain train_text, train_label, dev_text, dev_label, etc.\n",
        "\n",
        "To get pretrained w2v embeddings, we can use the ```gensim``` library. You can do\n",
        "\n",
        "```!pip install gensim```\n",
        "\n",
        "to get it first.\n",
        "\n",
        "One you installed the library, you can take a look at which pretrained embeddings are available for your to download.\n",
        "\n",
        "```\n",
        "import gensim.downloader\n",
        "#Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))\n",
        "\n",
        "```\n",
        "\n",
        "You should see a list of available pretrained embeddings like this:\n",
        "\n",
        "```\n",
        "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
        "```\n",
        "\n",
        "We recommend trying out a few, like the 'glove-wiki-gigaword-300' and 'word2vec-google-news-300'. To download the embeddings:\n",
        "\n",
        "```\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n",
        "```\n",
        "\n",
        "Once you downloaded it into a variable, you can do many things. For instance, you can find the most similar words to a query word you put in:\n",
        "\n",
        "```\n",
        "glove_vectors.most_similar('how')\n",
        "```\n",
        "\n",
        "You can also look at the embedding of a word:\n",
        "\n",
        "```\n",
        "word = \"how\"\n",
        "word_embedding = glove_vectors[word]\n",
        "\n",
        "```\n",
        "\n",
        "In tfidf, a sentence or document is naturally represented as a vector by the vocabulary based vectors. However, in w2v, you have a vector for each word, but not a sentence (alternatively, you can use something called doc2vec to directly encode a sentence). The most common way to get a sentence vector from word vectors is just to go through each word, get their embeddings and finally take an average of all word embeddings. If each word is a 300-d vector, then the final sentence vector is also 300-d.\n",
        "\n",
        "### Task 1: Write a ```get_sentence_embedding()``` function.\n",
        "\n",
        "First, you need to write a function to get sentence embeddings from all words. Note that when you look up a word embedding in the pretrained w2v, there is no guarantee that that word is in the w2v dictionary. If not, then you will get an error when you look at that word. In your code, you should build in error handling to take care of this situation. If a word is not present in the dictionary, you should initialize it with a 300-d zero vector using ```numpy.zeros()```.\n",
        "\n",
        "For this task let's use the pre-trained google news 300-d vector."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5GgcL-te-FK",
        "outputId": "fc0f1abb-f1d3-4f9f-b406-e8e1a5b3c597"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvZGIwXvfAyT",
        "outputId": "3df80251-c8aa-4707-f3bd-989fe8f3aafb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "EllAx63bfHFT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors.most_similar('how')"
      ],
      "metadata": {
        "id": "I7HfqhAxfd4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbcef83d-0d35-48f0-f3ed-c2f9987c075e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('what', 0.6820360422134399),\n",
              " ('How', 0.6297600865364075),\n",
              " ('why', 0.5838741064071655),\n",
              " ('whether', 0.5470786690711975),\n",
              " ('exactly', 0.5468065142631531),\n",
              " ('so', 0.5422654151916504),\n",
              " ('howthe', 0.5206358432769775),\n",
              " ('way', 0.5168502330780029),\n",
              " ('really', 0.5107458829879761),\n",
              " ('Q.How', 0.5080464482307434)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"how\"\n",
        "word_embedding = glove_vectors[word]\n",
        "print(word_embedding.shape)"
      ],
      "metadata": {
        "id": "ThF9RP8Xfgf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cbea16-eb7e-4a51-fca2-810e80213446"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XB1_TvGHeX_I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def get_sentence_embedding(sentence:str,glove_vectors)->np.ndarray:\n",
        "    \"\"\"\n",
        "    function to get embedding of a sentence from the words in it using w2v\n",
        "\n",
        "    args:\n",
        "        sentence: the input sentence to compute embeddings from\n",
        "        glove_vectors: the pretrained w2v object where you can look up word embeddings\n",
        "    returns:\n",
        "        a numpy ndarray with the same dimension as the pretrained w2v embeddings\n",
        "    \"\"\"\n",
        "    # YOU CODE HERE\n",
        "\n",
        "    sentence_list = sentence.split()\n",
        "    sentence_embedding = []\n",
        "\n",
        "    for word in sentence_list:\n",
        "        if word in glove_vectors:\n",
        "            word_embedding = glove_vectors[word]\n",
        "            sentence_embedding.append(word_embedding)\n",
        "\n",
        "    if sentence_embedding:\n",
        "        # the mean of all word embeddings in the sentence\n",
        "        sentence_embedding = np.mean(sentence_embedding, axis=0)\n",
        "    else:\n",
        "        # a word is not present in the dictionary\n",
        "        sentence_embedding = np.zeros(300)\n",
        "\n",
        "    return sentence_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YsrNCdPeX_J"
      },
      "source": [
        "### Task 2: encode your input sentences from train and test portion of the Triage dataset into vector representations\n",
        "\n",
        "Last week we saw how to use tf-idf vectors to represent sentences and use them in a classifier. Here we just need to similarly turn all training and dev sentences into vectors, but using w2v.\n",
        "\n",
        "Make use of the function above and go through all sentences in your train data and dev data. One possibility is that all of the words in a sentence may be absent from your pretrained w2v dictionary. In that case, it would just come out as a zero vector for the whole sentence, which may not be ideal but let's keep it simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dJXsMRxJeX_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f141ccee-1aca-46d3-b563-c81f2b327631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21046, 300)\n",
            "(2573, 300)\n"
          ]
        }
      ],
      "source": [
        "# Write code to go through all sentences in the train and dev data respectively\n",
        "# and encode them into vectors using the function you wrote above with w2v\n",
        "# Make sure the final matrix for your training set and dev set are represented in\n",
        "# numpy arrays, not list of lists.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "from util import load_data, Dataset, Example\n",
        "import numpy as np\n",
        "dataset = load_data(\"./data/triage\")\n",
        "\n",
        "def get_data(split:list[Example])->(list[str],list[int]):\n",
        "    \"\"\"\n",
        "    massage the data into a format consistent with the input type required by CountVectorizer or tfidfVectorizer.\n",
        "\n",
        "    args\n",
        "        split: pass in the split, which should be either dataset.train or dataset.dev\n",
        "\n",
        "    returns:\n",
        "        text: list of sentences\n",
        "        labels: list of labels\n",
        "\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # pass\n",
        "    text = list()\n",
        "    label = list()\n",
        "\n",
        "    for doc in split:\n",
        "        text.append(doc.words)\n",
        "        label.append(doc.label)\n",
        "\n",
        "    text = [[' '.join(i)][0] for i in text]\n",
        "\n",
        "    return text, label\n",
        "\n",
        "train_text, train_label = get_data(dataset.train)\n",
        "dev_text, dev_label = get_data(dataset.dev)\n",
        "\n",
        "train_encoded = [get_sentence_embedding(doc, glove_vectors) for doc in train_text]\n",
        "dev_encoded = [get_sentence_embedding(doc, glove_vectors) for doc in dev_text]\n",
        "\n",
        "train_encoded = np.array(train_encoded)\n",
        "dev_encoded = np.array(dev_encoded)\n",
        "print(train_encoded.shape)\n",
        "print(dev_encoded.shape)\n",
        "\n",
        "# all of the words in a sentence are absent from pretrained w2v dictionary\n",
        "# a zero vector - convert nan to 0\n",
        "train_encoded[np.isnan(train_encoded)] = 0\n",
        "dev_encoded[np.isnan(dev_encoded)] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQEdZL4neX_K"
      },
      "source": [
        "### Task 3: Logistic regression text classification with w2v\n",
        "\n",
        "Feed your w2v encoded train data into the logistic regression classifier you worked with last week, except this time you should use the scikit-learn built-in function of logistic regression. Report the accuracy for train and dev datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "59iA-RhVeX_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a378dc-20cc-4d5e-801a-838a7a23c71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train data: 0.7662263613038107\n",
            "Accuracy on dev data: 0.7625340069957248\n"
          ]
        }
      ],
      "source": [
        "# code for logistic regression with scikit-learn library.\n",
        "import sklearn.linear_model\n",
        "clf = sklearn.linear_model.LogisticRegression(random_state=101)\n",
        "\n",
        "clf.fit(train_encoded, train_label)\n",
        "train_pred = clf.predict(train_encoded)\n",
        "dev_pred = clf.predict(dev_encoded)\n",
        "\n",
        "assert(len(train_pred) == len(train_label))\n",
        "print(\"Accuracy on train data:\", sum(train_pred == train_label) / len(train_label))\n",
        "assert(len(dev_pred) == len(dev_label))\n",
        "print(\"Accuracy on dev data:\", sum(dev_pred == dev_label) / len(dev_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr3iedWaeX_L"
      },
      "source": [
        "## PART II: Train your own w2v embeddimgs on the Triage training data and test it on the dev data\n",
        "\n",
        "In this part we will train our w2v model based on the training dataset. First, you can read through the gensim package tutorial. Pay special attention to the ```training parameters section``` to understand the parameters in the ```Word2Vec``` function below.\n",
        "\n",
        "Assuming you have the ```train_text``` variable set up above, which is a list of sentences, we would still need to break each sentence into a list of words. In the below code, we first do that, then take the three steps to train a w2v model:\n",
        "\n",
        "1. initialize model with ```Word2Vec()```\n",
        "2. build your vocab\n",
        "3. train the model.\n",
        "\n",
        "### Task 3: train w2v model with default parameters\n",
        "\n",
        "using the code below, and then use your above code to feed your text training data and dev data to your logistic regression model with this new trained w2v dictionary. Note that to load the embeddings for a word, you need to look it up by:\n",
        "\n",
        "```word_emb = w2v_vector.wv[word]```\n",
        "\n",
        "Which is a little different from the pre-trained model.\n",
        "\n",
        "After training your logistic regression model, report accuracy for both training and dev data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OaDFaSNmeX_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c356e389-f454-4c55-ebe7-de139987ffc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11073529, 16206368)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import logging\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 8\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "sentences = [s.split() for s in train_text]\n",
        "\n",
        "w2v_model = Word2Vec(vector_size=W2V_SIZE,\n",
        "                    window=W2V_WINDOW,\n",
        "                    min_count=W2V_MIN_COUNT,\n",
        "                    workers=8)\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "w2v_model.train(sentences, total_examples=len(sentences), epochs=W2V_EPOCH, report_delay=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UXj8uQL-eX_L"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# to train a logistic regression model with your new w2v embeddings\n",
        "# report accuracy for training and dev data\n",
        "\n",
        "def get_sentence_embedding(sentence:str, w2v_model, W2V_SIZE)->np.ndarray:\n",
        "    \"\"\"\n",
        "    function to get embedding of a sentence from the words in it using w2v\n",
        "\n",
        "    args:\n",
        "        sentence: the input sentence to compute embeddings from\n",
        "        glove_vectors: the pretrained w2v object where you can look up word embeddings\n",
        "    returns:\n",
        "        a numpy ndarray with the same dimension as the pretrained w2v embeddings\n",
        "    \"\"\"\n",
        "    # YOU CODE HERE\n",
        "    # pass\n",
        "    sentence_list = sentence.split()\n",
        "    sentence_embedding = []\n",
        "\n",
        "    for word in sentence_list:\n",
        "        if word in w2v_model.wv:\n",
        "            word_embedding = w2v_model.wv[word]\n",
        "            sentence_embedding.append(word_embedding)\n",
        "\n",
        "    if sentence_embedding:\n",
        "        # the mean of all word embeddings in the sentence\n",
        "        sentence_embedding = np.mean(sentence_embedding, axis=0)\n",
        "    else:\n",
        "        # a word is not present in the dictionary\n",
        "        sentence_embedding = np.zeros(W2V_SIZE) # use W2V_SIZE instead of a constant\n",
        "\n",
        "    return sentence_embedding\n",
        "\n",
        "train_encoded_new = [get_sentence_embedding(doc, w2v_model, W2V_SIZE) for doc in train_text]\n",
        "dev_encoded_new = [get_sentence_embedding(doc, w2v_model, W2V_SIZE) for doc in dev_text]\n",
        "\n",
        "train_encoded_new = np.array(train_encoded_new)\n",
        "dev_encoded_new = np.array(dev_encoded_new)\n",
        "\n",
        "train_encoded_new[np.isnan(train_encoded_new)] = 0\n",
        "dev_encoded_new[np.isnan(dev_encoded_new)] = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = sklearn.linear_model.LogisticRegression(random_state=101, max_iter=500) # tried larger max_iter so the model can converge\n",
        "\n",
        "clf.fit(train_encoded_new, train_label)\n",
        "train_pred_new = clf.predict(train_encoded_new)\n",
        "dev_pred_new = clf.predict(dev_encoded_new)\n",
        "\n",
        "assert(len(train_pred_new) == len(train_label))\n",
        "print(\"Accuracy on train data:\", sum(train_pred_new == train_label) / len(train_label))\n",
        "assert(len(dev_pred_new) == len(dev_label))\n",
        "print(\"Accuracy on dev data:\", sum(dev_pred_new == dev_label) / len(dev_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvSm1IZ8iprU",
        "outputId": "0381d596-b9d1-49b7-adce-32438b5dda20"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train data: 0.7572935474674523\n",
            "Accuracy on dev data: 0.7532063738826272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_3GjJ1leX_L"
      },
      "source": [
        "### Task 3.1: play with hyperparameters\n",
        "\n",
        "Change the hyperparameters such as vector_size, window, min_count, etc., and train your w2v model again. Does the accuracy change? Report your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_OdjfHVkeX_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe00dc8-2366-4db8-8afa-fa7da649c080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W2V_SIZE: 100 W2V_WINDOW: 2 W2V_EPOCH: 8 W2V_MIN_COUNT: 2\n",
            "Accuracy on train data: 0.7234628908106053\n",
            "Accuracy on dev data: 0.7225029148853478\n",
            "W2V_SIZE: 100 W2V_WINDOW: 2 W2V_EPOCH: 8 W2V_MIN_COUNT: 10\n",
            "Accuracy on train data: 0.7211346574170864\n",
            "Accuracy on dev data: 0.7232802176447727\n",
            "W2V_SIZE: 100 W2V_WINDOW: 2 W2V_EPOCH: 32 W2V_MIN_COUNT: 2\n",
            "Accuracy on train data: 0.7449396559916374\n",
            "Accuracy on dev data: 0.7454333462883793\n",
            "W2V_SIZE: 100 W2V_WINDOW: 2 W2V_EPOCH: 32 W2V_MIN_COUNT: 10\n",
            "Accuracy on train data: 0.7408058538439608\n",
            "Accuracy on dev data: 0.7516517683637777\n",
            "W2V_SIZE: 100 W2V_WINDOW: 8 W2V_EPOCH: 8 W2V_MIN_COUNT: 2\n",
            "Accuracy on train data: 0.7259336691057683\n",
            "Accuracy on dev data: 0.7271667314418966\n",
            "W2V_SIZE: 100 W2V_WINDOW: 8 W2V_EPOCH: 8 W2V_MIN_COUNT: 10\n",
            "Accuracy on train data: 0.7198042383350756\n",
            "Accuracy on dev data: 0.7232802176447727\n",
            "W2V_SIZE: 100 W2V_WINDOW: 8 W2V_EPOCH: 32 W2V_MIN_COUNT: 2\n",
            "Accuracy on train data: 0.7453197757293547\n",
            "Accuracy on dev data: 0.7442673921492421\n",
            "W2V_SIZE: 100 W2V_WINDOW: 8 W2V_EPOCH: 32 W2V_MIN_COUNT: 10\n",
            "Accuracy on train data: 0.7434666920079825\n",
            "Accuracy on dev data: 0.7493198600855033\n",
            "W2V_SIZE: 300 W2V_WINDOW: 2 W2V_EPOCH: 8 W2V_MIN_COUNT: 2\n",
            "Accuracy on train data: 0.7234628908106053\n",
            "Accuracy on dev data: 0.7252234745433346\n",
            "W2V_SIZE: 300 W2V_WINDOW: 2 W2V_EPOCH: 8 W2V_MIN_COUNT: 10\n",
            "Accuracy on train data: 0.7177610947448446\n",
            "Accuracy on dev data: 0.7252234745433346\n",
            "W2V_SIZE: 300 W2V_WINDOW: 2 W2V_EPOCH: 32 W2V_MIN_COUNT: 2\n",
            "Accuracy on train data: 0.7542525895657132\n",
            "Accuracy on dev data: 0.7547609794014769\n",
            "W2V_SIZE: 300 W2V_WINDOW: 2 W2V_EPOCH: 32 W2V_MIN_COUNT: 10\n",
            "Accuracy on train data: 0.753112230352561\n",
            "Accuracy on dev data: 0.7520404197434901\n",
            "W2V_SIZE: 300 W2V_WINDOW: 8 W2V_EPOCH: 8 W2V_MIN_COUNT: 2\n",
            "Accuracy on train data: 0.7246032500237575\n",
            "Accuracy on dev data: 0.7252234745433346\n",
            "W2V_SIZE: 300 W2V_WINDOW: 8 W2V_EPOCH: 8 W2V_MIN_COUNT: 10\n",
            "Accuracy on train data: 0.7199942982039342\n",
            "Accuracy on dev data: 0.7190050524679362\n",
            "W2V_SIZE: 300 W2V_WINDOW: 8 W2V_EPOCH: 32 W2V_MIN_COUNT: 2\n",
            "Accuracy on train data: 0.7583388767461751\n",
            "Accuracy on dev data: 0.7543723280217645\n",
            "W2V_SIZE: 300 W2V_WINDOW: 8 W2V_EPOCH: 32 W2V_MIN_COUNT: 10\n",
            "Accuracy on train data: 0.7541100446640692\n",
            "Accuracy on dev data: 0.7497085114652157\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "W2V_SIZE_list = [100, 300]\n",
        "W2V_WINDOW_list = [2, 8]\n",
        "W2V_EPOCH_list = [8, 32]\n",
        "W2V_MIN_COUNT_list = [2, 10]\n",
        "\n",
        "sentences = [s.split() for s in train_text]\n",
        "\n",
        "for W2V_SIZE in W2V_SIZE_list:\n",
        "  for W2V_WINDOW in W2V_WINDOW_list:\n",
        "    for W2V_EPOCH in W2V_EPOCH_list:\n",
        "      for W2V_MIN_COUNT in W2V_MIN_COUNT_list:\n",
        "        w2v_model = Word2Vec(vector_size=W2V_SIZE,\n",
        "                             window=W2V_WINDOW,\n",
        "                             min_count=W2V_MIN_COUNT,\n",
        "                             epochs=W2V_EPOCH,\n",
        "                             workers=8)\n",
        "\n",
        "        w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "        w2v_model.train(sentences, total_examples=len(sentences), epochs=W2V_EPOCH, report_delay=1)\n",
        "\n",
        "        train_encoded_new = [get_sentence_embedding(doc, w2v_model, W2V_SIZE) for doc in train_text]\n",
        "        dev_encoded_new = [get_sentence_embedding(doc, w2v_model, W2V_SIZE) for doc in dev_text]\n",
        "\n",
        "        train_encoded_new = np.array(train_encoded_new)\n",
        "        dev_encoded_new = np.array(dev_encoded_new)\n",
        "\n",
        "        train_encoded_new[np.isnan(train_encoded_new)] = 0\n",
        "        dev_encoded_new[np.isnan(dev_encoded_new)] = 0\n",
        "\n",
        "        clf = sklearn.linear_model.LogisticRegression(random_state=101, max_iter=500) # tried larger max_iter so the model can converge\n",
        "\n",
        "        clf.fit(train_encoded_new, train_label)\n",
        "        train_pred_new = clf.predict(train_encoded_new)\n",
        "        dev_pred_new = clf.predict(dev_encoded_new)\n",
        "\n",
        "        print('W2V_SIZE:', W2V_SIZE, 'W2V_WINDOW:', W2V_WINDOW, 'W2V_EPOCH:', W2V_EPOCH, 'W2V_MIN_COUNT:', W2V_MIN_COUNT)\n",
        "        assert(len(train_pred_new) == len(train_label))\n",
        "        print(\"Accuracy on train data:\", sum(train_pred_new == train_label) / len(train_label))\n",
        "        assert(len(dev_pred_new) == len(dev_label))\n",
        "        print(\"Accuracy on dev data:\", sum(dev_pred_new == dev_label) / len(dev_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results shown above are the 16 combinations of different values of hyperparameters: vector_size, window, min_count, and epochs. The obtained accuracies are not significantly different, they are all approximately within the range of 72% to 76%."
      ],
      "metadata": {
        "id": "eYrxHNFTIe7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, they do have some level of impact. To be specific, **vector_size** is the dimensionality of the word vectors, which means larger vector sizes can capture more information from the text, so with suitable hyperparameter combinations (e.g., W2V_WINDOW: 8 W2V_EPOCH: 32 W2V_MIN_COUNT: 10), the models with vector_size=300 can achieve higher accuracies. **Window** is the maximum distance between the current and predicted word within a sentence, and a larger window captures broader context, so it appears that in this case, models with a smaller window size achieved higher accuracies on dev data. **Min_count** is the minimum total frequency, in some cases shown above, models with a high min_count achieved slightly lower accuracies on train data and slightly higher accuracies on dev data, showing setting min_count can help exclude noise. **Epochs** determine the number of iterations, in this case, models with more epochs achieved higher accuracies, which means they are getting better embeddings than the models with fewer epochs without overfitting."
      ],
      "metadata": {
        "id": "mWNncAHbbXI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to note that, due to limited computing power, only a limited number of combinations are included, so the conclusions are by no means definitive or comprehensive. The hyperparameters should be chosen according to specific tasks, and more combinations are needed to find the optimal ones."
      ],
      "metadata": {
        "id": "k6FCeLknbYfj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws_qygGyeX_M"
      },
      "source": [
        "## Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEATju8yeX_M"
      },
      "source": [
        "Compile the scores from the models and their input vectors from these three weeks. Compare the train and dev accuracy for these configurations:\n",
        "\n",
        "- NB\n",
        "- logistic regression (LR) with unigram count vectors\n",
        "- LR with unigram+bigram count vectors\n",
        "- LR with tfidf vectors\n",
        "- LR with pre-trained w2v vectors\n",
        "- LR with custom trained w2v vectors\n",
        "\n",
        "And analyze the accuracy results from train and dev data. What do you see in terms comparing different methods and input representations? What do you see in terms of train and dev accuracy trends?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iElaqG3neX_M"
      },
      "source": [
        "\n",
        "- **NB**: train accuracy ≈ 83%, dev accuracy ≈ 73%\n",
        "- **LR with unigram count vectors**: train accuracy ≈ 80%, dev accuracy ≈ 74%\n",
        "- **LR with unigram+bigram count vectors**: train accuracy ≈ 98%, dev accuracy ≈ 76%\n",
        "- **LR with tfidf vectors**: train accuracy ≈ 93%, dev accuracy ≈ 76%\n",
        "- **LR with pre-trained w2v vectors**: train accuracy ≈ 77%, dev accuracy ≈ 76%\n",
        "- **LR with custom-trained w2v vectors**: train accuracy ≈ 76%, dev accuracy ≈ 75%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In terms of input representations, models with **simple count-based methods** have relatively lower dev accuracies. Models with more complex feature representations such as **TF-IDF** and **unigram+bigram** appear to be able to fit the training data very well but have higher risks of overfitting, as they have very high training accuracies but their dev accuracies are not significantly higher than the other models, suggesting weighting terms by their importance or including both unigrams and bigrams may help capture more information from training set but the models are not generalized enough to show a matching increase in dev accuracy. **Pre-trained w2v vectors** are powerful and capture semantic information, and in this case, it is the most suitable as the model performs well on both train data and dev data. **Custom-trained w2v vectors** have similar but slightly inferior performance compared to pre-trained w2v vectors, which means the text in this problem is not too unique or domain-specific, thus making custom training unnecessary."
      ],
      "metadata": {
        "id": "dKYrKlvjZtVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze train and dev accuracy trends, **NB** is the simplest model with reasonable training and dev accuracies. **LR with unigram count vectors** has slightly higher dev accuracy and lower training accuracy than NB, which means this model performs slightly better and is more generalized than NB. **LR with unigram+bigram count vectors** has higher dev accuracy and significantly higher training accuracy, suggesting this model is overfitting. Similarly, **LR with tfidf vectors** has similar dev accuracy to that of LR with unigram+bigram count vectors and an extremely high training accuracy as well, indicating this model is also overfitting. **LR with pre-trained w2v vectors** demonstrates the best performance, as it has similar dev accuracy to those of the previous two models, while maintaining similar accuracies on train data and dev data, showing it can fit the training set and generalize on the dev data very well. **LR with custom-trained w2v vectors** has only slightly worse accuracies than using pre-trained w2v vectors, it is possible that with more hyperparameter tuning in the custom training process, this model can achieve better performance."
      ],
      "metadata": {
        "id": "FUtrYbLpPv7y"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp-class-2023",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}