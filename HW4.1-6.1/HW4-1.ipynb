{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP HWs 4.1 Text Classification with NB\n",
    "\n",
    "In these three homeworks, each week you will use one more model or input representation to perform text classification on the same datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util contains data loading functions and classes\n",
    "from util import load_data, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm # show progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4.1 Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Implement NB from scratch\n",
    "\n",
    "### Details about the Triage dataset\n",
    "\n",
    "The documents in our dataset are either text messages, social media (Twitter) posts, or snippets from news articles. In addition to the specific events listed above the dataset contains a number of news articles spanning dozens of different disasters. All messages have been translated and annotated by humans on the crowdsourcing platform CrowdFlower (now branded under Appen). However, some of the translations are not perfect, and you may encounter some words in other languages. Unfortunately, NLP researchers often have to work with messy data. If you are curious about the crowdsourcing translation effort for messages from Haiti in particular, feel free to check out this paper (https://nlp.stanford.edu/pubs/munro2010translation.pdf).\n",
    "\n",
    "<b>Your task is to classify each document as being aid-related, class AID, or not aid-related, class NOT.</b> Messages that are aid-related include individuals' requests for food, water, or shelter etc. The aid class also includes news reports about dire situations and disaster relief efforts. \n",
    "\n",
    "<b>Training and Validation sets.</b> The data is divided into a training set, development (validation) set, and test set. Recall that the training set is used to learn, compute the statistics for, your model. These statistics are then used to classify the documents in the development and test sets. For this assignment, you should train on the training set and test your model on both the train and dev set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset exploration\n",
    "\n",
    "We use classes defined in ```util.py``` to load data and labels. Take a look at that module to have a deeper understanding of what's in each class. Here are some examples usages to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'util.Example'>\n",
      "21046\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from util import load_data\n",
    "dataset = load_data(\"./data/triage\")\n",
    "\n",
    "# explore the dataset class\n",
    "# dataset contains dataset.train and dataset.dev\n",
    "train_data = dataset.train\n",
    "\n",
    "# train_data is a list of items of type Example (defined in util.py. there are 21046 train examples)\n",
    "print(type(train_data))\n",
    "print(type(train_data[0]))\n",
    "print(len(train_data))\n",
    "\n",
    "# you can do the same to explore dataset.dev\n",
    "# you should use only dataset.train for training\n",
    "# and you can test your model on both train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'util.Example'>\n",
      "2573\n"
     ]
    }
   ],
   "source": [
    "# development (validation) set\n",
    "dev_data = dataset.dev\n",
    "\n",
    "print(type(dev_data))\n",
    "print(type(dev_data[0]))\n",
    "print(len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['thisisjoej', 'earthquake', 'after', 'shock', 'hahaha', 'did', 'i', 'do', 'it', 'right', 'joe']\n",
      "label: 0\n",
      "words: ['thanks', 'to', 'an', 'umbrella', 'grant', 'from', 'the', 'us', 'agency', 'of', 'international', \"development's\", 'office', 'of', 'foreign', 'disaster', 'assistance', 'usaid', 'ofda', 'umcor', 'is', 'reaching', 'the', 'most', 'vulnerable', 'to', 'provide', '60', '000', 'emergency', 'kits', 'that', 'include', 'water', 'glucose', 'and', 'biscuits', 'as', 'well', 'as', 'help', 'build', '4', '400', 'emergency', 'shelters', 'and', '800', 'duplex', 'toilets', 'for', 'displaced', 'people', 'fleeing', 'to', 'vavuniya']\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "# look at each example, let's look at the first one\n",
    "first_data_point = train_data[0]\n",
    "\n",
    "# each example has two parts: the words and label. \n",
    "print(\"words:\",first_data_point.words)\n",
    "print(\"label:\",first_data_point.label)\n",
    "\n",
    "# look at another example\n",
    "fifth_data_point = train_data[5]\n",
    "\n",
    "# each example has two parts: the words and label. \n",
    "print(\"words:\",fifth_data_point.words)\n",
    "print(\"label:\",fifth_data_point.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing NB\n",
    "\n",
    "In our textbook SLP3, chapter 4 (https://web.stanford.edu/~jurafsky/slp3/4.pdf), Section 4.2 describes training and testing a NB model. In this exercise, follow chapter 4.2 and the algorithm outlined in Figure 4.2 to implement Naive Bayes algorithm to perform text classification on the data set provided in the ```data``` folder. I've included the screen shots of these algirthms outlines. You can read the textbook to get more detailed description. \n",
    "\n",
    "#### Implement train function using this algorithm from Figure 4.2 in SLP3\n",
    "\n",
    "<img src=\"img/function2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "#### Implement inference function using this algorithm from Figure 4.2 in SLP3\n",
    "<img src=\"img/function1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(dataset:Dataset,C=[0,1]) -> (dict, dict, set):\n",
    "    \"\"\"\n",
    "    implement this function according to the algorithm outlined above. \n",
    "\n",
    "    for classes C, 1 is AID, 0 is NOT, as described above. \n",
    "\n",
    "    return log_prior, log_likelihood, and V as specified in the algorithm.\n",
    "    \"\"\"\n",
    "    # pass\n",
    "    log_prior = dict() # {0: -0.532, 1: -0.885}\n",
    "    log_likelihood = dict()\n",
    "    \n",
    "    N_doc = len(dataset.train) # number of documents in D, should be 21046\n",
    "    \n",
    "    V = set() # vocabulary of D\n",
    "    bigdoc = {0: list(), 1: list()}\n",
    "    for doc in tqdm(dataset.train, desc =\"Calculating V & bigdoc\"):\n",
    "        bigdoc[doc.label].extend(doc.words) # should use extend not append\n",
    "        for word in doc.words:\n",
    "            V.add(word)\n",
    "\n",
    "    for c in tqdm(C, desc =\"Calculating log_prior & log_likelihood\"):\n",
    "        N_c = sum(1 for doc in dataset.train if doc.label == c) # number of documents from D in class c\n",
    "        log_prior[c] = np.log(N_c / N_doc)\n",
    "        \n",
    "        for w in V:\n",
    "            count_w = bigdoc[c].count(w) # number of occurrences of w in bigdoc[c]\n",
    "            log_likelihood[(w, c)] = np.log((count_w + 1) / (len(bigdoc[c]) + len(V)))\n",
    "    \n",
    "    return log_prior, log_likelihood, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_inference(test_doc:list, log_prior:dict, log_likelihood:dict, C:list, V:set) -> int:\n",
    "    \"\"\"\n",
    "    implement this function to make an inference on a test example. it should return an integer, 0 or 1, these are the two possible classes in the dataset. \n",
    "    \n",
    "    the test_doc argument is represented in the Example class using the words attribute, e.g., in above example in dataset exploration, the test_doc input would be first_data_point.words, which is a list of words\n",
    "\n",
    "    the other arguments of this function, log_prior, log_likelihood, C, and V are all seen above in the trainNB() function. \n",
    "    \n",
    "    \"\"\"\n",
    "    # pass\n",
    "    sum_C = [0, 0]\n",
    "    for c in C:\n",
    "        sum_C[c] = log_prior[c]\n",
    "        for w in test_doc:\n",
    "            if w in V:\n",
    "                sum_C[c] += log_likelihood[(w, c)]\n",
    "    return np.argmax(sum_C) # class 0 or class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training NB classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating V & bigdoc: 100%|██████████| 21046/21046 [00:00<00:00, 212438.56it/s]\n",
      "Calculating log_prior & log_likelihood: 100%|██████████| 2/2 [04:41<00:00, 140.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7329965021375826\n",
      "accuracy 0.82946878266654\n"
     ]
    }
   ],
   "source": [
    "# example inference pipeline usage to evaluate your classifier\n",
    "# you can run this as it is, or you are free to add more things to it. \n",
    "\n",
    "def testNB(split, log_prior, log_likelihood, V, C):\n",
    "    \"\"\"\n",
    "    argument: split can be dataset.train or dataset.dev\n",
    "    \"\"\"\n",
    "    inferences = []\n",
    "    for d in split: # each document\n",
    "        result = NB_inference(d.words, log_prior, log_likelihood, C, V)\n",
    "        inferences.append(result)\n",
    "    preds = np.array(inferences) # predicted\n",
    "    gts = np.array([d.label for d in split]) # actual\n",
    "    assert(len(preds) == len(gts))\n",
    "    print(\"accuracy\", sum(preds == gts) / len(gts))\n",
    "\n",
    "dataset = load_data(\"./data/triage\")\n",
    "C = [0,1]\n",
    "log_prior, log_likelihood, V = trainNB(dataset)\n",
    "\n",
    "# evaluate your model on dev and train\n",
    "testNB(dataset.dev, log_prior, log_likelihood, V, C)\n",
    "testNB(dataset.train, log_prior, log_likelihood, V, C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "1. when you train the model it can take more than a minute. It's a good idea to use a progress bar to track your training progress. you can use https://github.com/tqdm/tqdm\n",
    "\n",
    "2. the expected accuracy on dev data is about 73% and on train data is about 83%. If you are around that number, you should be good to go.\n",
    "\n",
    "3. in the ```util.py``` there are more functions and classes that is currently not used in this notebook. If you want to make use of them, feel free to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Extra credit (worth extra 20% of this assignment)\n",
    "\n",
    "Use the ```english.stop``` in the ```data``` folder to remove stop words and then train again to see if your accuracy is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stop words\n",
    "\n",
    "file_path = \"./data/english.stop\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    stop_words = set(file.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_trainNB function to remove stop words\n",
    "\n",
    "def new_trainNB(dataset:Dataset,C=[0,1]) -> (dict, dict, set):\n",
    "    \"\"\"\n",
    "    implement this function according to the algorithm outlined above. \n",
    "\n",
    "    for classes C, 1 is AID, 0 is NOT, as described above. \n",
    "\n",
    "    return log_prior, log_likelihood, and V as specified in the algorithm.\n",
    "    \"\"\"\n",
    "    log_prior = dict()\n",
    "    log_likelihood = dict()\n",
    "    \n",
    "    N_doc = len(dataset.train) # number of documents in D, should be 21046\n",
    "    \n",
    "    V = set() # vocabulary of D\n",
    "    bigdoc = {0: list(), 1: list()}\n",
    "    for doc in tqdm(dataset.train, desc =\"Calculating V & bigdoc\"):\n",
    "        \n",
    "        no_stopwords = [w for w in doc.words if w not in stop_words] # keep the words that are not stop words\n",
    "        \n",
    "        bigdoc[doc.label].extend(no_stopwords) # stop words removed\n",
    "        for word in no_stopwords:\n",
    "            V.add(word)\n",
    "\n",
    "    for c in tqdm(C, desc =\"Calculating log_prior & log_likelihood\"):\n",
    "        N_c = sum(1 for doc in dataset.train if doc.label == c) # number of documents from D in class c\n",
    "        log_prior[c] = np.log(N_c / N_doc)\n",
    "        \n",
    "        for w in V:\n",
    "            count_w = bigdoc[c].count(w) # number of occurrences of w in bigdoc[c]\n",
    "            log_likelihood[(w, c)] = np.log((count_w + 1) / (len(bigdoc[c]) + len(V)))\n",
    "    \n",
    "    return log_prior, log_likelihood, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating V & bigdoc: 100%|██████████| 21046/21046 [00:00<00:00, 192474.69it/s]\n",
      "Calculating log_prior & log_likelihood: 100%|██████████| 2/2 [02:52<00:00, 86.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7306645938593082\n",
      "accuracy 0.8446735721752352\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data(\"./data/triage\")\n",
    "C = [0,1]\n",
    "log_prior, log_likelihood, V = new_trainNB(dataset) # use the new function for training\n",
    "# NB_inference and testNB functions are the same\n",
    "\n",
    "# evaluate your model on dev and train\n",
    "testNB(dataset.dev, log_prior, log_likelihood, V, C)\n",
    "testNB(dataset.train, log_prior, log_likelihood, V, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new accuracy on dev data is still about 73% (slightly lower) and the new accuracy on train data is about 84% (slightly higher). Removing the stop words does not seem to have much impact on the accuracy in this case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
